# **📘 PRJ4-2025-Ind-MorrisWu – Personal Development Plan (Week 1–12)**

---

## 🎯 **LO1: Plans and executes a project in the data track in an agile way (Level 2 HBO-i)**

---

### 📅 **Week 1 – Getting Started with Python**

**Summary**:
I laid the groundwork for my data skills by focusing on Python basics such as data structures (lists, dictionaries) and conditional logic. These skills are essential for any further data manipulation and analysis.

**Activities**:

* Completed beginner-level exercises on Python syntax
* Practiced `if-else` statements, loops, and dictionary manipulations

**Deliverables**:

* Basic Python scripts implementing control flow and data structures
  🔗 [Evidence](https://images.hahow.in/images/664ed9e92fcb5581a2712953)

**LO2 Practice**:

* **Personal Leadership**: Established consistent weekly study slots and stuck to the schedule
* **Future-Oriented Organization**: Set up Git-based file and folder structure to manage project materials effectively

**Reflection**:
Starting with the fundamentals helped me realize how much can be achieved with simple constructs. Organizing my environment early was crucial, and learning version control prepared me for collaborative work and future scalability.

---

### 📅 **Week 2 – Diving into Loops and Functions**

**Summary**:
I progressed to control flow structures and modular programming using functions, as well as learned basic exception handling with `try/except`.

**Activities**:

* Built scripts using loops (`for`, `while`)
* Practiced writing custom functions
* Handled exceptions using `try/except`

**Deliverables**:

* Multiple Python exercises using loops and functions
  🔗 [Evidence](https://images.hahow.in/images/66768214fb7a2272b4ba139a)

**LO2 Practice**:

* **Personal Leadership**: Defined clear goals for code quality and progress milestones
* **Investigative Ability**: Researched how Python internally handles errors and stack traces

**Reflection**:
This week taught me the importance of abstraction. Writing clean, reusable functions significantly improved the readability and maintainability of my code. Debugging via error tracing also laid the foundation for better testing practices.

---

### 📅 **Week 3 – Data Tools + Project Brainstorming**

**Summary**:
I began using **NumPy** and **Pandas** for preliminary data analysis and explored ideas for the Canon project, focusing on identifying potential data points and business objectives.

**Activities**:

* Manipulated Series and DataFrames
* Brainstormed Canon project directions with structured notes

**Deliverables**:

* Project planning notes and Python data exploration exercises
  🔗 [Evidence](https://github.com/FontysVenlo/PRJ4-2025-D02/blob/main/Data%20Understanding%20copy.ipynb)

**LO2 Practice**:

* **Future-Oriented Organization**: Identified long-term data needs for the Canon project
* **Personal Leadership**: Took initiative to connect business challenges with technical capabilities

**Reflection**:
Exploring Pandas marked a turning point from pure coding to thinking in terms of datasets and use cases. I also began seeing how data could be structured to reflect real-world business logic.

---

### 📅 **Week 4 – Mini Project: Data Cleaning + Visualization**

**Summary**:
Using a sample dataset, I practiced cleaning real-world data and creating visualizations to gain preliminary insights. This included dealing with missing values and redundant entries.

**Activities**:

* Performed `dropna`, `groupby`, and basic aggregations
* Visualized trends using Seaborn and Matplotlib

**Deliverables**:

* A visual report summarizing dataset quality and key patterns
  🔗 [Evidence](https://github.com/FontysVenlo/PRJ4-2025-D02/blob/main/Data%20Understanding%20copy.ipynb)

**LO2 Practice**:

* **Investigative Ability**: Analyzed outliers and NA values for meaning
* **Targeted Interaction**: Shared results with peers to evaluate the clarity of visuals

**Reflection**:
This hands-on experience with messy data made me appreciate the time-consuming yet critical nature of cleaning processes. It also underscored how visual communication impacts stakeholder understanding.

---

### 📅 **Week 5 – Project Framework + Business Context**

**Summary**:
I worked on formally defining the business case for the Canon project, including key stakeholders, problem statements, and metrics.

**Activities**:

* Completed a project canvas
* Outlined KPIs and user profiles

**Deliverables**:

* Canon project plan and Notion documentation
  🔗 [Evidence](https://www.notion.so/Business-Analysis-Structure-1b8f94066b8080d6a8a7e08eda0e8d54?pvs=4)

**LO2 Practice**:

* **Future-Oriented Organization**: Mapped technical insights to business outcomes
* **Investigative Ability**: Conducted background research on Canon’s maintenance and error-reporting processes

**Reflection**:
This planning phase emphasized the importance of grounding data efforts in real business problems. It was challenging but rewarding to think like both an analyst and a stakeholder.

---

### 📅 **Week 6 – Unit Testing + Code Structure**

**Summary**:
I finalized the Python course with a focus on writing unit tests using `unittest`. I ensured the reliability and correctness of utility functions by validating them through test coverage.

**Activities**:

* Developed testable code blocks
* Used assertions to catch logical errors

**Deliverables**:

* Python scripts with full test coverage
  🔗 [Evidence](https://images.hahow.in/images/670e013310af204e9e444a30)

**LO2 Practice**:

* **Targeted Interaction**: Wrote clean docstrings and test documentation
* **Investigative Ability**: Debugged failure cases using error logs and assertions

**Reflection**:
Testing taught me how to think ahead and anticipate edge cases. Writing reusable, validated code allowed me to scale confidently into more complex modules for the main project.

---

### 📅 **Week 7 – Custom Functions & TDD Practice**

**Summary**:
I refined my approach by writing helper functions for filtering and parsing. I applied Test-Driven Development (TDD) to validate logic as I wrote it.

**Activities**:

* Wrote utility functions from scratch
* Implemented full unit test suites for each module

**Deliverables**:

* Verified scripts and reusable code blocks
  🔗 [Evidence](https://github.com/FontysVenlo/PRJ4-2025-D02/blob/main/Data%20Understanding%20copy.ipynb)

**LO2 Practice**:

* **Personal Leadership**: Proactively designed tools for future tasks
* **Targeted Interaction**: Clear code annotation to aid collaboration

**Reflection**:
Practicing TDD sharpened my thinking around inputs, outputs, and robustness. It instilled a habit of thinking about usability and clarity beyond just making something “work.”

---

### 📅 **Week 8 – Canon Project Implementation (Phase 1)**

**Summary**:
I began implementing the Canon analysis logic, focusing on pipeline design, feature preparation, and simulating typical input-output workflows.

**Activities**:

* Established script templates and logic flows
* Integrated filtering logic for different copier models

**Deliverables**:

* Initial codebase simulating filter + error data flows
  🔗 [Evidence](https://github.com/FontysVenlo/PRJ4-2025-D02/blob/main/Data%20Understanding%20copy.ipynb)

**LO2 Practice**:

* **Future-Oriented Organization**: Built for scalability and modularity
* **Investigative Ability**: Traced dependencies between sensor data and reported errors

**Reflection**:
This week marked a move from exploration to real implementation. Designing the pipeline forced me to balance technical feasibility with data limitations.

---

### 📅 **Week 9 – Exploratory Data Analysis (EDA)**

**Summary**:
Conducted a detailed data analysis phase to understand correlations between flow changes and error types.

**Activities**:

* Created histograms, heatmaps, and scatter plots
* Analyzed feature importance and missing value patterns

**Deliverables**:

* Annotated Jupyter Notebook with statistical findings
  🔗 [Evidence](https://github.com/FontysVenlo/PRJ4-2025-D02/blob/main/Data%20Understanding%20copy.ipynb)

**LO2 Practice**:

* **Investigative Ability**: Identified key signal patterns from error logs
* **Targeted Interaction**: Documented findings clearly for future reference

**Reflection**:
Understanding the data revealed potential inconsistencies in error logging. It also prepared the ground for meaningful feature engineering in the modeling stage.

---

### 📅 **Week 10 – Model Development (Phase 1)**

**Summary**:
Developed baseline models to predict error types based on flow variables. Evaluated initial results using accuracy and F1 scores.

**Activities**:

* Trained classifiers on preprocessed data
* Compared metrics to identify a starting point

**Deliverables**:

* Initial predictive model
  🔗 [Evidence](https://github.com/FontysVenlo/PRJ4-2025-D02/blob/main/Model.ipynb)

**LO2 Practice**:

* **Personal Leadership**: Managed modeling timeline efficiently
* **Future-Oriented Organization**: Modularized code for retraining and reuse

**Reflection**:
Creating a functioning model was a major milestone. It was also a humbling reminder that accuracy alone is not enough — interpretability and business relevance matter too.

---

### 📅 **Week 11 – Model Tuning and Evaluation**

**Summary**:
Improved the model through hyperparameter tuning and added cross-validation to ensure performance generalizes across unseen data.

**Activities**:

* Applied GridSearchCV and confusion matrices
* Performed stratified cross-validation

**Deliverables**:

* Tuned model with evaluation report
  🔗 [Evidence](https://github.com/FontysVenlo/PRJ4-2025-D02/blob/main/Model.ipynb)

**LO2 Practice**:

* **Investigative Ability**: Interpreted confusion matrices and recall metrics
* **Future-Oriented Organization**: Integrated tuning into pipeline design

**Reflection**:
Tuning taught me how even small parameter shifts can impact results. I also learned to look deeper into metrics beyond accuracy to understand the model’s true performance.

---

### 📅 **Week 12 – Final Optimization and Error Correction**

**Summary**:
Focused on refining error label mismatches and finalized all pipeline components for deployment or final presentation.

**Activities**:

* Investigated misclassifications
* Adjusted thresholds and revalidated the model logic

**Deliverables**:

* Final version of the model with corrected labeling issues
  🔗 [Evidence](https://github.com/FontysVenlo/PRJ4-2025-D02/blob/main/Model.ipynb)

**LO2 Practice**:

* **Investigative Ability**: Mapped false positives to root causes
* **Targeted Interaction**: Documented final logic with clarity for reviewers

**Reflection**:
This final phase was crucial to polishing the project. Catching edge-case label issues improved trust in the model and underlined the value of continual testing, even post-evaluation.
